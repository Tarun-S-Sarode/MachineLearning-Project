{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH2319 Machine Learning Project Phase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan Prediction Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names: Poorva Shetye(s3732674) & Tarun Sarode(s3700754)\n",
    "June 9, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Abstract\n",
    "\n",
    "# 2  Introduction\n",
    "\n",
    ">2.1 Dataset\n",
    "\n",
    ">2.2 Target Feature\n",
    "\n",
    ">2.3 Descriptive Features\n",
    "\n",
    "# 3   Methodology\n",
    "\n",
    ">3.1 Data Preparation and Exploration\n",
    "\n",
    ">3.2 Encoding\n",
    "\n",
    ">3.2.1 Encoding target variable\n",
    "\n",
    ">3.2.2 Encoding Categorical Features\n",
    "\n",
    ">3.3 caling of Features\n",
    "\n",
    ">3.4 Feature Selection & Ranking\n",
    "\n",
    ">3.5 Train-Test Splitting\n",
    "\n",
    ">3.6 Model Evaluation Strategy\n",
    "\n",
    ">3.7 Hyperparameter Tuning\n",
    "\n",
    ">3.8 Model Fitting\n",
    "\n",
    ">3.8.1 K-Nearest Neighbour\n",
    "\n",
    ">3.8.2 (Gaussian) Naive Bayes (NB)\n",
    "\n",
    ">3.8.3 Decision Tree (DT)\n",
    "\n",
    "# 4     Results\n",
    "\n",
    ">  Performance Comparison\n",
    "\n",
    "# 5     Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 1 Abstract\n",
    "\n",
    "The aim of this project is to predict whether a person is eligible for a loan on basis of various descriptive features.\n",
    "\n",
    "The dataset is taken from https://www.kaggle.com/sanket5/loan-prediction-binary-class-problem/notebook.\n",
    "\n",
    "This project has two phases:\n",
    "\n",
    "-In phase 1, we did the basic data Pre-processing and detailed descriptive statistical analysis of the data \n",
    "\n",
    "-In phase 2, we are applying different ML classification algorithms to predict if the person is eligible for loan.\n",
    "\n",
    "This report includes phase 2 with explanation as well as the Python codes for better understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Dataset\n",
    "\n",
    "The Kaggle dataset contains 2 datasets i.e., train.csv and test.csv. We would be considering only the train dataset since test dataset does not contain the target feature in the dataset, which is a variable we aim to predict. The train dataset has 614 observations. It has 12 descriptive features and 1 target feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Target Feature\n",
    "\n",
    "The target feature is Loan_Status which is a categorial feature.It has 2 classes i.e. Y or N, which makes it a binary classification. The aim is to predict whether the person gets the loan.Loan_Status- Yes, No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Descriptive Features\n",
    "\n",
    "The variable description for the train.csv dataset are as follows:\n",
    "\n",
    "Loan_ID: Unique ID for every person.\n",
    "\n",
    "Gender: The gender attribute is divided into Male and Female.\n",
    "\n",
    "Married: Yes, No.\n",
    "\n",
    "Dependents: There are continuous.\n",
    "\n",
    "It defines the number of dependents.\n",
    "\n",
    "Education:states whether the person was a Graduate or not.\n",
    "\n",
    "Self_Employed: Yes, No.\n",
    "\n",
    "ApplicantIncome : continuous.\n",
    "\n",
    "It states the income of the person applying for the loan.\n",
    "\n",
    "CoapplicantIncome: continuous.\n",
    "\n",
    "It states the income of the co-applicant of the person applying for the loan.\n",
    "\n",
    "LoanAmount: continuous.\n",
    "\n",
    "The amount that the loan has been applied for.\n",
    "\n",
    "Loan_Amount_Term: continuous.\n",
    "\n",
    "The number of years that the person wants to apply loan for.\n",
    "\n",
    "Credit_History: continuous.\n",
    "\n",
    "It states whether the person is credible for loan.\n",
    "\n",
    "Property_Area: Rural, Semi-urban, Urban.It states whether the person stays in the mentioned areas.\n",
    "\n",
    "All the descriptive features are almost self-explanatory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Methodology\n",
    "\n",
    "The very first important thing before carrying any of the process is to ensure that the data set is free of any noise in it, which makes the models to predict accurately and correctly.\n",
    "\n",
    "In machine learning we have different ways to predict using different models which are supervised, unsupervised and semi-supervised Machine learning techniques.\n",
    "\n",
    "In our project we are using the supervised Machine learning techniques which inc;ude Classification and Regression techniques.\n",
    "\n",
    "We are appliying the following Classification techiniques in this project:\n",
    "\n",
    "> K-Nearest Neighbours(KNN).\n",
    "\n",
    "> Decision Tree(DT).\n",
    "\n",
    "> Gaussian Naive Bayes(NB).\n",
    "\n",
    "Before fitting the above models,we encode all the features to numeric because all the ML algorithm requires all the input and output variables to be numeric.\n",
    "\n",
    "The most important step is to select the best features using the powerful Random Forest Importance.\n",
    "\n",
    "Next using feature selection together with hyperparameter search, we conduct a 5-fold stratified cross-validation to fine-tune hyperparameters of each classifier using Area Under Curve (AUC) as the performance metric.\n",
    "\n",
    "We build each model using parallel processing with \"-2\" cores. \n",
    "\n",
    "Next, stratification is crucial to ensure that each validation set has the same proportion of classes as in the original dataset.Using the hyperparameters search, the best model on the test data is identified for each of the classifiers.\n",
    "\n",
    "After the best model is identified, we apply the 10-fold cross-validation on the test data and then conduct paired t-test to note whether any performance difference is is statistically significant.\n",
    "\n",
    "Finally, we compare the results of all the three classifiers using the 'Confusion Matrix','Precision' and 'Recall'. which gives us the accuracy of the prediction and concludes the best model which can be used on the test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Data Preparation and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data preparation and exploration is carried in Phase 1, but the data Preparation is also done in phase 2 as it is essential to carry on the main process.\n",
    "\n",
    "We will start by loading the CSV data from the file (using appropriate pandas functions) and checking whether the loaded data is equivalent to the data in the source CSV file.\n",
    "\n",
    "The train.csv dataset is imported:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] URL = ( https://www.kaggle.com/sanket5/loan-prediction-dataset ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas as pd\n",
    "a = pd.read_csv('train.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Loan_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LP001002</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LP001003</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>4583</td>\n",
       "      <td>1508.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LP001005</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LP001006</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Not Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>2583</td>\n",
       "      <td>2358.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LP001008</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Loan_ID Gender Married Dependents     Education Self_Employed  \\\n",
       "0  LP001002   Male      No          0      Graduate            No   \n",
       "1  LP001003   Male     Yes          1      Graduate            No   \n",
       "2  LP001005   Male     Yes          0      Graduate           Yes   \n",
       "3  LP001006   Male     Yes          0  Not Graduate            No   \n",
       "4  LP001008   Male      No          0      Graduate            No   \n",
       "\n",
       "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
       "0             5849                0.0         NaN             360.0   \n",
       "1             4583             1508.0       128.0             360.0   \n",
       "2             3000                0.0        66.0             360.0   \n",
       "3             2583             2358.0       120.0             360.0   \n",
       "4             6000                0.0       141.0             360.0   \n",
       "\n",
       "   Credit_History Property_Area Loan_Status  \n",
       "0             1.0         Urban           Y  \n",
       "1             1.0         Rural           N  \n",
       "2             1.0         Urban           Y  \n",
       "3             1.0         Urban           Y  \n",
       "4             1.0         Urban           Y  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the first 5 rows of the dataset\n",
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we would change the attributes names to simplify them.\n",
    "a.columns=['loanid','gender','married','dependents','education','self-employed','income', 'co-applicant-income','loan-amount','loan-duration','credit-history','property-area','loan-status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(614, 13)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking if the data is loaded as per the source file.\n",
    "a1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loanid</th>\n",
       "      <th>gender</th>\n",
       "      <th>married</th>\n",
       "      <th>dependents</th>\n",
       "      <th>education</th>\n",
       "      <th>self-employed</th>\n",
       "      <th>income</th>\n",
       "      <th>co-applicant-income</th>\n",
       "      <th>loan-amount</th>\n",
       "      <th>loan-duration</th>\n",
       "      <th>credit-history</th>\n",
       "      <th>property-area</th>\n",
       "      <th>loan-status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LP001002</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LP001003</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>4583</td>\n",
       "      <td>1508.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LP001005</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LP001006</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Not Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>2583</td>\n",
       "      <td>2358.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LP001008</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     loanid gender married dependents     education self-employed  income  \\\n",
       "0  LP001002   Male      No          0      Graduate            No    5849   \n",
       "1  LP001003   Male     Yes          1      Graduate            No    4583   \n",
       "2  LP001005   Male     Yes          0      Graduate           Yes    3000   \n",
       "3  LP001006   Male     Yes          0  Not Graduate            No    2583   \n",
       "4  LP001008   Male      No          0      Graduate            No    6000   \n",
       "\n",
       "   co-applicant-income  loan-amount  loan-duration  credit-history  \\\n",
       "0                  0.0          NaN          360.0             1.0   \n",
       "1               1508.0        128.0          360.0             1.0   \n",
       "2                  0.0         66.0          360.0             1.0   \n",
       "3               2358.0        120.0          360.0             1.0   \n",
       "4                  0.0        141.0          360.0             1.0   \n",
       "\n",
       "  property-area loan-status  \n",
       "0         Urban           Y  \n",
       "1         Rural           N  \n",
       "2         Urban           Y  \n",
       "3         Urban           Y  \n",
       "4         Urban           Y  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again loading the first 5 rows of the dataset after changing the names of the attributes\n",
    "a1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loanid                  object\n",
       "gender                  object\n",
       "married                 object\n",
       "dependents              object\n",
       "education               object\n",
       "self-employed           object\n",
       "income                   int64\n",
       "co-applicant-income    float64\n",
       "loan-amount            float64\n",
       "loan-duration          float64\n",
       "credit-history         float64\n",
       "property-area           object\n",
       "loan-status             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the datatypes of each attribute column\n",
    "a1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we would change the datatype of income column from int64 to float64 as it gives better results\n",
    "a1['income'] = a1['income'].astype(int).astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the data set is (614, 13) \n",
      "\n",
      "Data types are: \n",
      "loanid                  object\n",
      "gender                  object\n",
      "married                 object\n",
      "dependents              object\n",
      "education               object\n",
      "self-employed           object\n",
      "income                 float64\n",
      "co-applicant-income    float64\n",
      "loan-amount            float64\n",
      "loan-duration          float64\n",
      "credit-history         float64\n",
      "property-area           object\n",
      "loan-status             object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# We will again check the dimension and datatypes of each column after checking the type of the income column\n",
    "print(f\"Dimension of the data set is {a.shape} \\n\")\n",
    "print(f\"Data types are: \")\n",
    "print(a1.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are using strip to remove the white spaces which is a good practice to ensure that there is no white spaces in the datasset.\n",
    "\n",
    "a1['loanid']=a1['loanid'].str.strip(' ')\n",
    "a1['gender']=a1['gender'].str.strip(' ')\n",
    "a1['married']=a1['married'].str.strip(' ')\n",
    "a1['dependents']=a1['dependents'].str.strip(' ')\n",
    "a1['education']=a1['education'].str.strip(' ')\n",
    "a1['self-employed']=a1['self-employed'].str.strip(' ')\n",
    "a1['property-area']=a1['property-area'].str.strip(' ')\n",
    "a1['loan-status']=a1['loan-status'].str.strip(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loanid                  0\n",
       "gender                 13\n",
       "married                 3\n",
       "dependents             15\n",
       "education               0\n",
       "self-employed          32\n",
       "income                  0\n",
       "co-applicant-income     0\n",
       "loan-amount            22\n",
       "loan-duration          14\n",
       "credit-history         50\n",
       "property-area           0\n",
       "loan-status             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking whether all the NaN values for the continuous features have been replaced\n",
    "a1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-26-4372149a4019>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# Replacing all the categorical attributes with mode\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0ma1\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'self-employed'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0ma1\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'self-employed'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreplace\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mNaN\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0ma1\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'self-employed'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0ma1\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'dependents'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0ma1\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'dependents'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreplace\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mNaN\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0ma1\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'dependents'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0ma1\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'gender'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0ma1\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'gender'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreplace\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mNaN\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0ma1\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'gender'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0ma1\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'married'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0ma1\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'married'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreplace\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mNaN\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0ma1\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'married'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\satis\\desktop\\pycharm\\mlproject\\venu\\lib\\site-packages\\pandas\\core\\series.py\u001B[0m in \u001B[0;36mreplace\u001B[1;34m(self, to_replace, value, inplace, limit, regex, method)\u001B[0m\n\u001B[0;32m   4567\u001B[0m             \u001B[0mlimit\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlimit\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4568\u001B[0m             \u001B[0mregex\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mregex\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 4569\u001B[1;33m             \u001B[0mmethod\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmethod\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   4570\u001B[0m         )\n\u001B[0;32m   4571\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\satis\\desktop\\pycharm\\mlproject\\venu\\lib\\site-packages\\pandas\\core\\generic.py\u001B[0m in \u001B[0;36mreplace\u001B[1;34m(self, to_replace, value, inplace, limit, regex, method)\u001B[0m\n\u001B[0;32m   6542\u001B[0m                         \u001B[0mdest_list\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   6543\u001B[0m                         \u001B[0minplace\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minplace\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 6544\u001B[1;33m                         \u001B[0mregex\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mregex\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   6545\u001B[0m                     )\n\u001B[0;32m   6546\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\satis\\desktop\\pycharm\\mlproject\\venu\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001B[0m in \u001B[0;36mreplace_list\u001B[1;34m(self, src_list, dest_list, inplace, regex)\u001B[0m\n\u001B[0;32m    660\u001B[0m                         \u001B[0minplace\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minplace\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    661\u001B[0m                         \u001B[0mconvert\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconvert\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 662\u001B[1;33m                         \u001B[0mregex\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mregex\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    663\u001B[0m                     )\n\u001B[0;32m    664\u001B[0m                     \u001B[1;32mif\u001B[0m \u001B[0mm\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0many\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mconvert\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\satis\\desktop\\pycharm\\mlproject\\venu\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001B[0m in \u001B[0;36m_replace_coerce\u001B[1;34m(self, to_replace, value, inplace, regex, convert, mask)\u001B[0m\n\u001B[0;32m   2604\u001B[0m                 \u001B[0mregex\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mregex\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2605\u001B[0m                 \u001B[0mconvert\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconvert\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2606\u001B[1;33m                 \u001B[0mmask\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmask\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2607\u001B[0m             )\n\u001B[0;32m   2608\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mconvert\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\satis\\desktop\\pycharm\\mlproject\\venu\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001B[0m in \u001B[0;36m_replace_coerce\u001B[1;34m(self, to_replace, value, inplace, regex, convert, mask)\u001B[0m\n\u001B[0;32m   1504\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mregex\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1505\u001B[0m                 \u001B[0mself\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcoerce_to_target_dtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1506\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mputmask\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmask\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minplace\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minplace\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1507\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1508\u001B[0m                 return self._replace_single(\n",
      "\u001B[1;32mc:\\users\\satis\\desktop\\pycharm\\mlproject\\venu\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001B[0m in \u001B[0;36mputmask\u001B[1;34m(self, mask, new, inplace, axis, transpose)\u001B[0m\n\u001B[0;32m    919\u001B[0m         \"\"\"\n\u001B[0;32m    920\u001B[0m         \u001B[0mmask\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_extract_bool_array\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmask\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 921\u001B[1;33m         \u001B[1;32massert\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnew\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mABCIndexClass\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mABCSeries\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mABCDataFrame\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    922\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    923\u001B[0m         \u001B[0mnew_values\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m  \u001B[1;31m# delay copy if possible.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAssertionError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Replacing all the categorical attributes with mode\n",
    "a1['self-employed']=a1['self-employed'].replace([np.NaN],[a1['self-employed'].mode()])\n",
    "a1['dependents']=a1['dependents'].replace([np.NaN],[a1['dependents'].mode()])\n",
    "a1['gender']=a1['gender'].replace([np.NaN],[a1['gender'].mode()])\n",
    "a1['married']=a1['married'].replace([np.NaN],[a1['married'].mode()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['col'].values[df['col'].values > x] = y #y is the value to be replaced.\n",
    "\n",
    "#income\n",
    "# replacing the outliers by mean for values > 30000\n",
    "a1['income'].values[a1['income'].values > 30000] = a1['income'].mean()\n",
    "\n",
    "#co-applicant-income\n",
    "# replacing by mean for values > 10000\n",
    "a1['co-applicant-income'].values[a1['co-applicant-income'].values > 10000] = a1['co-applicant-income'].mean()\n",
    "\n",
    "#loan amount\n",
    "# replacing by mean for values > 10000\n",
    "a1['loan-amount'].values[a1['loan-amount'].values > 400] = a1['loan-amount'].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the NaN values with mean\n",
    "a1['loan-duration']=a1['loan-duration'].replace([np.NaN],[a1['loan-duration'].mean()])\n",
    "a1['loan-amount']=a1['loan-amount'].replace([np.NaN],[a1['loan-amount'].mean()])\n",
    "a1['credit-history']=a1['credit-history'].replace([np.NaN],[a1['credit-history'].mode()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether all the NaN values have been replaced\n",
    "a1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary Statistics\n",
    "\n",
    "The summary of the entire data is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2.1 Encoding target variable\n",
    "\n",
    "Since the target is binary we are replacing it with 1 and 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data1 = a1.drop(columns='loan-status')\n",
    "target1 = a1['loan-status']\n",
    "target1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target1 = target1.replace({'Y': 1, 'N': 0})\n",
    "target1.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2.2 Encoding Categorical Features\n",
    "\n",
    "Prior to modeling, it is essential to encode all categorical features (both the target feature and the descriptive features) into a set of numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data2 = Data1.drop(columns='loanid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning the Data 2 to categorical_cols to encode\n",
    "categorical_cols = Data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding all the categorical columns\n",
    "for col in categorical_cols:\n",
    "    n = len(Data2[col].unique())\n",
    "    if (n == 2):\n",
    "        Data2[col] = pd.get_dummies(Data2[col], drop_first=True)\n",
    "\n",
    "# we are using one-hot-encoding for categorical features with >2 levels and converting it into binary levels (0,1)\n",
    "Data3 = pd.get_dummies(Data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data3.sample(5, random_state=999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Scaling of Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After encoding all the categorical features, we perform the scaling of the descriptive feature to be between 0 and 1 before fitting any classifiers.\n",
    "\n",
    "We can use the following scaling methods\n",
    "\n",
    ">Min-Max Scaling\n",
    ">Standard Scaling\n",
    ">Robust Scaling\n",
    "\n",
    "we wil be using the Robust scaling, since we have outliers in our dataset.\n",
    "\n",
    "Before scaling,we will make a copy of our data to keep track of the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "Data4 = Data3.copy()\n",
    "\n",
    "Data_scaler = preprocessing.RobustScaler()\n",
    "Data_scaler.fit(Data4)\n",
    "Data5 = Data_scaler.fit_transform(Data4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Data5, columns=Data4.columns).sample(5, random_state=999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Feature Selection & Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is one of the steps which can help us to predict accurately because the features in real have effect on the target feature and selecting the best features becomes very important which can be done using following methods:\n",
    "\n",
    ">F-Score(a statistical filter method)\n",
    "\n",
    ">Mutual information (an entropy-based filter method)\n",
    "\n",
    ">Random forest importance (an ensemble-based filter method)\n",
    "\n",
    ">SPSA (a new wrapper method developed by V. Aksakalli et al.)\n",
    "\n",
    "Here we are using the Random Forest Importance which rates the features from highest to lowest on the basis of their importance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It gives the top best features according to their importance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "num_features = 16\n",
    "model_rfi = RandomForestClassifier(n_estimators=100)\n",
    "model_rfi.fit(Data5, target1)\n",
    "fs_indices_rfi = np.argsort(model_rfi.feature_importances_)[::-1][0:num_features]\n",
    "\n",
    "best_features_rfi = Data4.columns[fs_indices_rfi].values\n",
    "best_features_rfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gives the best features rates based on their importance\n",
    "feature_importances_rfi = model_rfi.feature_importances_[fs_indices_rfi]\n",
    "feature_importances_rfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we are using the altair to declare the links between data columns and visual encoding channels\n",
    "import altair as alt\n",
    "from vega_datasets import data\n",
    "alt.renderers.enable('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_imp(best_features, scores, method_name, color):\n",
    "    \n",
    "    df = pd.DataFrame({'features': best_features, \n",
    "                       'importances': scores})\n",
    "    \n",
    "    chart = alt.Chart(df, \n",
    "                      width=500, \n",
    "                      title=method_name + ' Feature Importances'\n",
    "                     ).mark_bar(opacity=0.85, \n",
    "                                color=color).encode(\n",
    "        alt.X('features', title='Feature', sort=None, axis=alt.AxisConfig(labelAngle=45)),\n",
    "        alt.Y('importances', title='Importance')\n",
    "    )\n",
    "    \n",
    "    return chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_imp(best_features_rfi, feature_importances_rfi, 'Random Forest', 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above visualisation we can see that the first highest important feature is 'credit history',\n",
    "followed by 'income', 'loan amount' and 'co applicant income'\n",
    "followed by the remaining features.\n",
    "\n",
    "From this we are considering the best features to be 1, 4, 5 10 and all the features of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5  Train-Test Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 614 observations. We would split the descriptive features and the target feature into train and test partitions with a 70:30 ratio using stratification.That is, we use 70 % of the data to build a classifier and evaluate its performance on the test set of 30 %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data5train, data5test,target1train, target1test = train_test_split(Data5, target1, \n",
    "                                                    test_size = 0.3, random_state=10,\n",
    "                                                    stratify = target1)\n",
    "\n",
    "print(data5train.shape)\n",
    "print(data5test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the 'train' data set has 429 observations with all the features and the 'test' data set has 185 observations with all the features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6 Model Evaluation Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train and tune our models on 429 rows of training data and we will test them on 185 rows of test data i.e, 70% of the data is for building KNN classifier and the performance evaluation is obtained with the test data. \n",
    "\n",
    "The 5-fold stratified cross-validation evaluation method is used for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "cv_method = StratifiedKFold(n_splits=5, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.7 Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the Pipeline, we stack feature selection and grid search for KNN hyperparameter tuning via cross-validation.\n",
    "\n",
    "We will use the same Pipeline methodology for NB and DT.\n",
    "\n",
    "The KNN hyperparameters are as follows:\n",
    "\n",
    ">number of neighbors (n_neighbors) \n",
    "\n",
    ">the distance metric p.\n",
    "\n",
    ">number of features (n_features)\n",
    "\n",
    "For feature selection, we use the powerful Random Forest Importance (RFI) method with 100 estimators. A trick here is that we need a bit of coding so that we can make RFI feature selection as part of the pipeline. For this reason, we define the custom RFIFeatureSelector() class below to pass in RFI as a \"step\" to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# custom function for RFI feature selection inside a pipeline\n",
    "# here we use n_estimators=100\n",
    "class RFIFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    # class constructor \n",
    "    # make sure class attributes end with a \"_\"\n",
    "    # per scikit-learn convention to avoid errors\n",
    "    def __init__(self, n_features_=16):\n",
    "        self.n_features_ = n_features_\n",
    "        self.fs_indices_ = None\n",
    "\n",
    "    # override the fit function\n",
    "    def fit(self, X, y):\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from numpy import argsort\n",
    "        model_rfi = RandomForestClassifier(n_estimators=100)\n",
    "        model_rfi.fit(X, y)\n",
    "        self.fs_indices_ = argsort(model_rfi.feature_importances_)[::-1][0:self.n_features_] \n",
    "        return self \n",
    "    \n",
    "    # override the transform function\n",
    "    def transform(self, X, y=None):\n",
    "        return X[:, self.fs_indices_]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.8 Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.8.1 K-Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "pipe_KNN = Pipeline(steps=[('rfi_fs', RFIFeatureSelector()), \n",
    "                           ('knn', KNeighborsClassifier())])\n",
    "\n",
    "params_pipe_KNN = {'rfi_fs__n_features_': [1, 4, 5, 10, Data5.shape[1]],\n",
    "                   'knn__n_neighbors': [1, 5, 10, 15, 20, 50],\n",
    "                   'knn__p': [1, 2]}\n",
    "\n",
    "gs_pipe_KNN = GridSearchCV(estimator=pipe_KNN, \n",
    "                           param_grid=params_pipe_KNN, \n",
    "                           cv=cv_method,\n",
    "                           refit=True,\n",
    "                           n_jobs=-2,\n",
    "                           scoring='roc_auc',\n",
    "                           verbose=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fit the classifier on the train data and evaluate its performance on the test data. \n",
    "First fit the nearest neighbor classifier on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the 5 fold \n",
    "gs_pipe_KNN.fit(data5train,target1train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best parameters\n",
    "gs_pipe_KNN.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the best performing parameters are \n",
    "\n",
    ">features - 4\n",
    "\n",
    ">p - 1\n",
    "\n",
    ">neighbours - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction score of the Knn\n",
    "gs_pipe_KNN.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nearest neighbor classifier scores an accuracy rate of 74% in this particular case on the train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though these are the best values, let's have a look at the other combinations to see if the difference is rather significant or not. For this, we will make use of the function below to format the grid search outputs as a Pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom function to format the search results as a Pandas data frame\n",
    "def get_search_results(gs):\n",
    "\n",
    "    def model_result(scores, params):\n",
    "        scores = {'mean_score': np.mean(scores),\n",
    "             'std_score': np.std(scores),\n",
    "             'min_score': np.min(scores),\n",
    "             'max_score': np.max(scores)}\n",
    "        return pd.Series({**params,**scores})\n",
    "\n",
    "    models = []\n",
    "    scores = []\n",
    "\n",
    "    for i in range(gs.n_splits_):\n",
    "        key = f\"split{i}_test_score\"\n",
    "        r = gs.cv_results_[key]        \n",
    "        scores.append(r.reshape(-1,1))\n",
    "\n",
    "    all_scores = np.hstack(scores)\n",
    "    for p, s in zip(gs.cv_results_['params'], all_scores):\n",
    "        models.append((model_result(s, p)))\n",
    "\n",
    "    pipe_results = pd.concat(models, axis=1).T.sort_values(['mean_score'], ascending=False)\n",
    "\n",
    "    columns_first = ['mean_score', 'std_score', 'max_score', 'min_score']\n",
    "    columns = columns_first + [c for c in pipe_results.columns if c not in columns_first]\n",
    "\n",
    "    return pipe_results[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_KNN = get_search_results(gs_pipe_KNN)\n",
    "results_KNN.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results we can see that the difference between the hyperparameter combinations is not really much when conditioned on the number of features selected.\n",
    "\n",
    "Let's visualize the results of the grid search corresponding to 4 selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "results_KNN_4_features = results_KNN[results_KNN['rfi_fs__n_features_'] == 4.0]\n",
    "\n",
    "alt.Chart(results_KNN_4_features, \n",
    "          title='KNN Performance Comparison with 4 Features'\n",
    "         ).mark_line(point=True).encode(\n",
    "    alt.X('knn__n_neighbors', title='Number of Neighbors'),\n",
    "    alt.Y('mean_score', title='AUC Score', scale=alt.Scale(zero=False)),\n",
    "    alt.Color('knn__p:N', title='p')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8.2 (Gaussian) Naive Bayes (NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement a Gaussian Naive Bayes model. As we do not have any prior information about our dataset, we will optimize thevar_smoothing (a variant of Laplace smoothing). By default, the var_smoothing parameter's value is  109  . \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "Data5train_transformed = PowerTransformer().fit_transform(data5train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "pipe_NB = Pipeline([('rfi_fs', RFIFeatureSelector()), \n",
    "                     ('nb', GaussianNB())])\n",
    "\n",
    "params_pipe_NB = {'rfi_fs__n_features_': [1, 4, 5, 10, Data5.shape[1]],\n",
    "                  'nb__var_smoothing': np.logspace(1,-3, num=200)}\n",
    "\n",
    "n_iter_search = 20\n",
    "gs_pipe_NB = RandomizedSearchCV(estimator=pipe_NB, \n",
    "                          param_distributions=params_pipe_NB, \n",
    "                          cv=cv_method,\n",
    "                          refit=True,\n",
    "                          n_jobs=-2,\n",
    "                          scoring='roc_auc',\n",
    "                          n_iter=n_iter_search,\n",
    "                          verbose=1) \n",
    "\n",
    "gs_pipe_NB.fit(Data5train_transformed, target1train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_pipe_NB.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_pipe_NB.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes predicts the correct labels on the train dataset with an accuracy rate of 76% which is higher than the KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though these are the results from the best values, let's have a look at the other combinations to see if the difference is rather significant or not. For this, we will make use of the function below to format the grid search outputs as a Pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_NB = get_search_results(gs_pipe_NB)\n",
    "results_NB.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results we can see that the difference between the hyperparameter combinations is not really much when conditioned on the number of features selected.\n",
    "\n",
    "Let's visualize the results of the grid search corresponding to 16 selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_NB_16_features = results_NB[results_NB['rfi_fs__n_features_'] == 16.0]\n",
    "\n",
    "alt.Chart(results_NB_16_features, \n",
    "          title='NB Performance Comparison with 16 Features'\n",
    "         ).mark_line(point=True).encode(\n",
    "    alt.X('nb__var_smoothing', title='Var. Smoothing'),\n",
    "    alt.Y('mean_score', title='AUC Score', scale=alt.Scale(zero=False))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8.3 Decision Trees (DT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a DT using gini index to maximize information gain. We aim to determine the optimal combinations of maximum depth (max_depth) and minimum sample split (min_samples_split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pipe_DT = Pipeline([('rfi_fs', RFIFeatureSelector()),\n",
    "                    ('dt', DecisionTreeClassifier(criterion='gini'))])\n",
    "\n",
    "params_pipe_DT = {'rfi_fs__n_features_': [1, 4, 5, 10, Data5.shape[1]],\n",
    "                  'dt__max_depth': [3, 4, 5],\n",
    "                  'dt__min_samples_split': [2, 5]}\n",
    "\n",
    "gs_pipe_DT = GridSearchCV(estimator=pipe_DT, \n",
    "                          param_grid=params_pipe_DT, \n",
    "                          cv=cv_method,\n",
    "                          refit=True,\n",
    "                          n_jobs=-2,\n",
    "                          scoring='roc_auc',\n",
    "                          verbose=1) \n",
    "\n",
    "gs_pipe_DT.fit(data5train, target1train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_pipe_DT.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gs_pipe_DT.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree predicts the correct labels on the train dataset with an accuracy rate of 71%. \n",
    "We observe that the accuracy of the Decision tree is slightly lower as compared to that of the nearest neighbor classifier and Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_DT = get_search_results(gs_pipe_DT)\n",
    "\n",
    "results_DT_4_features = results_DT[results_DT['rfi_fs__n_features_'] == 4.0]\n",
    "\n",
    "alt.Chart(results_DT_4_features, \n",
    "          title='DT Performance Comparison with 4 Features'\n",
    "         ).mark_line(point=True).encode(\n",
    "    alt.X('dt__min_samples_split', title='Min Samples for Split'),\n",
    "    alt.Y('mean_score', title='AUC Score', scale=alt.Scale(zero=False)),\n",
    "    alt.Color('dt__max_depth:N', title='Max Depth')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the optimal value of maximum depth hyperparameter is at 4 which is not at the end of its search space. \n",
    "\n",
    "Thus, we  do not need to go beyond what we already tried to make sure that we are not missing out on even better values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have optimized each one of the three classifiers using the training data. We now fit the optimized models on the test data in a cross-validated fashion. But since cross validation itself is a random process, we perform pairwise t-tests to determine if any difference between the performance of any two optimized classifiers is statistically significant \n",
    "\n",
    "First, we perform 10-fold stratified cross-validation on each best model (without any repetitions).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_method_ttest = StratifiedKFold(n_splits=10, random_state=111)\n",
    "\n",
    "cv_results_KNN = cross_val_score(estimator=gs_pipe_KNN.best_estimator_,\n",
    "                                 X=data5test,\n",
    "                                 y=target1test, \n",
    "                                 cv=cv_method_ttest, \n",
    "                                 n_jobs=-2,\n",
    "                                 scoring='roc_auc')\n",
    "cv_results_KNN.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5test_transformed = PowerTransformer().fit_transform(data5test)\n",
    "\n",
    "cv_results_NB = cross_val_score(estimator=gs_pipe_NB.best_estimator_,\n",
    "                                X=data5test_transformed,\n",
    "                                y=target1test, \n",
    "                                cv=cv_method_ttest, \n",
    "                                n_jobs=-2,\n",
    "                                scoring='roc_auc')\n",
    "cv_results_NB.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_DT = cross_val_score(estimator=gs_pipe_DT.best_estimator_,\n",
    "                                X=data5test,\n",
    "                                y=target1test, \n",
    "                                cv=cv_method_ttest, \n",
    "                                n_jobs=-2,\n",
    "                                scoring='roc_auc')\n",
    "cv_results_DT.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we conduct a paired t-test for the AUC score between the following model combinations:\n",
    "\n",
    ">KNN vs. NB,\n",
    "\n",
    ">KNN vs. DT, and\n",
    "\n",
    ">DT vs. NB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print(stats.ttest_rel(cv_results_KNN, cv_results_NB))\n",
    "print(stats.ttest_rel(cv_results_DT, cv_results_KNN))\n",
    "print(stats.ttest_rel(cv_results_DT, cv_results_NB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A p-value smaller than 0.05 indicates a statistically significant difference. Looking at these results, since all the values are higher than 0.05, amongst the 3 classifiers, Gaussian Naive Bayes model has the smallest value of 0.28 as compared to KNN and Decision tree model. Thus, \n",
    "\n",
    "Though we used AUC to optimize the algorithm hyperparameters, we shall consider the following metrics to evaluate models based on the test set:\n",
    "\n",
    ">Accuracy\n",
    "\n",
    ">Precision\n",
    "\n",
    ">Recall\n",
    "\n",
    ">F1 Score (the harmonic average of precision and recall)\n",
    "\n",
    ">Confusion Matrix\n",
    "\n",
    "These metrics can be computed using classification_report from sklearn.metrics. The classification reports are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_KNN = gs_pipe_KNN.predict(data5test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5transformed = PowerTransformer().fit_transform(data5test)\n",
    "pred_NB = gs_pipe_NB.predict(data5transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_DT = gs_pipe_DT.predict(data5test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrices are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#KNN\n",
    "#cm_vis = confusion_matrix(predicted,target_test,[1,0])\n",
    "cm_vis0 = confusion_matrix(pred_KNN,target1test)\n",
    "sns.heatmap(cm_vis0, annot=True, fmt='.2f' )\n",
    "#sns.heatmap(cm_vis)\n",
    "plt.ylabel('True class')\n",
    "plt.xlabel('Predicted class')\n",
    "plt.title('KNN')\n",
    "\n",
    "\n",
    "print(\"\\nConfusion matrix for K-Nearest Neighbor\") \n",
    "print(metrics.confusion_matrix(target1test, pred_KNN))\n",
    "\n",
    "print(\"\\nClassification report for K-Nearest Neighbor\") \n",
    "print(metrics.classification_report(target1test, pred_KNN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results of the KNN algorithm which has a score of 68.90% we can see that the figure table of confusion matrix indicates that for the people who got the loan(1) gave a precision of 79% and recall of 93%. \n",
    "Also, for the people who did not get the loan(0) gave  \n",
    "a precision and recall values as 75% and 47% respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB\n",
    "#cm_vis = confusion_matrix(predicted,target_test,[1,0])\n",
    "cm_vis1 = confusion_matrix(pred_NB,target1test)\n",
    "sns.heatmap(cm_vis0, annot=True, fmt='.2f' )\n",
    "#sns.heatmap(cm_vis)\n",
    "plt.ylabel('True class')\n",
    "plt.xlabel('Predicted class')\n",
    "plt.title('NB')\n",
    "\n",
    "\n",
    "print(\"\\nConfusion matrix for Naive Bayes\") \n",
    "print(metrics.confusion_matrix(target1test, pred_NB))\n",
    "\n",
    "print(\"\\nClassification report for Naive Bayes\") \n",
    "print(metrics.classification_report(target1test, pred_NB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results of the Naive Bayes algorithm which has a score of 75.21% we can see that the figure table of confusion matrix indicate that for the people who got the loan(1) gave a precision of 69% and recall of 100% and for the people who did not get the loan(0) gave  \n",
    "a precision and recall values as 0% and 0% respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DT\n",
    "#cm_vis = confusion_matrix(predicted,target_test,[1,0])\n",
    "cm_vis2 = confusion_matrix(pred_DT,target1test)\n",
    "sns.heatmap(cm_vis0, annot=True, fmt='.2f' )\n",
    "#sns.heatmap(cm_vis)\n",
    "plt.ylabel('True class')\n",
    "plt.xlabel('Predicted class')\n",
    "plt.title('DT')\n",
    "\n",
    "\n",
    "print(\"\\nConfusion matrix for Decision Tree\") \n",
    "print(metrics.confusion_matrix(target1test, pred_DT))\n",
    "\n",
    "print(\"\\nClassification report for Decision Tree\") \n",
    "print(metrics.classification_report(target1test, pred_DT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results of the Decision Tree algorithm which has a score of 71.81%, we can see that the figure table of confusion matrix indicates that for the people who got the loan(1) gave a precision of 77% and recall of 85% and for the people who did not get the loan(0) gave a precision and recall values as 57% and 43% respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our modeling using the Classification tecnique we can come to a conclussion for predicting weather a person recives a loan or not we can say that the applicants who got loan or not according to the features selected by Hyper parameter tuning we can say that the Gaussian Naive Bayes gave the highest score followed by Decission Tree and next by K-Nearest Neigbours. These results were compared by the comfussion matrix which gave the precission and recall of the model fitted. For all this we have used the Grid Search CV which includes all the parameters and gives the best fitted parameter to predict the result.\n",
    "\n",
    "The only limitation which we can see is that our data set has 614 observations which is less than compared to the real world data, yet we can assume this as a sample of the huge data and can impliment it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}